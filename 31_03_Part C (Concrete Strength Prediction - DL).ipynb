{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd4dc0fc-cc88-4705-86c2-3e7d3ff96706",
   "metadata": {},
   "source": [
    "#### C. Increate the number of epochs (5 marks)\r\n",
    "\r\n",
    "Repeat Part B but use 100 epochs this time for training.\r\n",
    "\r\n",
    "How does the mean of the mean squared errors compare to that from Step B?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "694cedff-153c-4309-a1d4-153b60abb119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np  # For numerical operations\n",
    "import pandas as pd  # For data manipulation\n",
    "\n",
    "from sklearn.metrics import mean_squared_error  # To calculate the Mean Squared Error\n",
    "from sklearn.model_selection import train_test_split  # To split the dataset into training and test sets\n",
    "from sklearn.preprocessing import StandardScaler  # To normalize the data\n",
    "from tensorflow.keras.layers import Dense  # To define the layers of the neural network\n",
    "from tensorflow.keras.models import Sequential  # To build the neural network\n",
    "from tensorflow.keras.optimizers import Adam  # Optimizer for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fedf3d14-aba1-499a-a1e3-ddd65480acaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "url = \"concrete_data.csv\"  # Path to the dataset\n",
    "data = pd.read_csv(url)  # Load the dataset into a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "055bb8a8-9498-4c01-9d90-ad08afbe7ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into predictors (X) and target variable (y)\n",
    "X = data.drop(\"Strength\", axis=1)  # Features/predictors (all columns except \"Strength\")\n",
    "y = data[\"Strength\"]  # Target variable (\"Strength\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f38f966b-e299-42fd-ac7e-de35402f662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part A: Build Baseline Model\n",
    "def baseline_model(X, y):\n",
    "    \"\"\"\n",
    "    Builds and evaluates a baseline regression model using Keras.\n",
    "    The model:\n",
    "    - Has one hidden layer with 10 nodes and ReLU activation.\n",
    "    - Uses the Adam optimizer and mean squared error loss function.\n",
    "    The process is repeated 50 times, and the mean and standard deviation of the MSEs are computed.\n",
    "\n",
    "    Parameters:\n",
    "    X: Features (predictors)\n",
    "    y: Target variable (concrete strength)\n",
    "\n",
    "    Returns:\n",
    "    Mean and standard deviation of the MSEs from 50 iterations.\n",
    "    \"\"\"\n",
    "    mse_list = []  # List to store MSEs from each iteration\n",
    "\n",
    "    # Repeat the process 50 times\n",
    "    for _ in range(50):\n",
    "        # Split the dataset into training (70%) and testing (30%) sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=np.random.randint(0, 100))\n",
    "        \n",
    "        # Build the neural network model\n",
    "        model = Sequential([\n",
    "            Dense(10, activation='relu', input_shape=(X_train.shape[1],)),  # Hidden layer with 10 nodes\n",
    "            Dense(1)  # Output layer with a single node (for regression)\n",
    "        ])\n",
    "        \n",
    "        # Compile the model using Adam optimizer and mean squared error loss\n",
    "        model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
    "        \n",
    "        # Train the model on the training data for 50 epochs\n",
    "        model.fit(X_train, y_train, epochs=50, verbose=0)\n",
    "        \n",
    "        # Evaluate the model on the test data\n",
    "        y_pred = model.predict(X_test, verbose=0)  # Predict on the test set\n",
    "        mse = mean_squared_error(y_test, y_pred)  # Calculate Mean Squared Error\n",
    "        mse_list.append(mse)  # Append the MSE to the list\n",
    "    \n",
    "    # Return the mean and standard deviation of the MSEs\n",
    "    return np.mean(mse_list), np.std(mse_list)\n",
    "\n",
    "# Part B: Normalize the data\n",
    "def normalized_model(X, y):\n",
    "    \"\"\"\n",
    "    Builds and evaluates a regression model using normalized data.\n",
    "    The normalization process:\n",
    "    - Subtracts the mean and divides by the standard deviation for each feature.\n",
    "    The rest of the process is the same as the baseline model, with 50 iterations.\n",
    "\n",
    "    Parameters:\n",
    "    X: Features (predictors)\n",
    "    y: Target variable (concrete strength)\n",
    "\n",
    "    Returns:\n",
    "    Mean and standard deviation of the MSEs from 50 iterations with normalized data.\n",
    "    \"\"\"\n",
    "    # Normalize the data using StandardScaler\n",
    "    scaler = StandardScaler()  # Initialize the scaler\n",
    "    X_normalized = scaler.fit_transform(X)  # Fit the scaler to X and transform it\n",
    "    \n",
    "    # Call the baseline model function with the normalized data\n",
    "    return baseline_model(X_normalized, y)\n",
    "\n",
    "# Part C: Increase the number of epochs\n",
    "def increased_epochs_model(X, y):\n",
    "    \"\"\"\n",
    "    Builds and evaluates a regression model with an increased number of epochs (100).\n",
    "    The model:\n",
    "    - Has one hidden layer with 10 nodes and ReLU activation.\n",
    "    - Uses the Adam optimizer and mean squared error loss function.\n",
    "    The dataset is normalized before training, and the process is repeated 50 times.\n",
    "\n",
    "    Parameters:\n",
    "    X: Features (predictors)\n",
    "    y: Target variable (concrete strength)\n",
    "\n",
    "    Returns:\n",
    "    Mean and standard deviation of the MSEs from 50 iterations with increased epochs.\n",
    "    \"\"\"\n",
    "    mse_list = []  # List to store MSEs from each iteration\n",
    "\n",
    "    # Normalize the dataset to zero mean and unit variance\n",
    "    scaler = StandardScaler()  # Initialize the StandardScaler\n",
    "    X_normalized = scaler.fit_transform(X)  # Fit and transform X\n",
    "\n",
    "    # Repeat the process 50 times for robust evaluation\n",
    "    for _ in range(50):\n",
    "        # Split the normalized dataset into training and testing sets (70% train, 30% test)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_normalized, y, test_size=0.3, random_state=np.random.randint(0, 100)\n",
    "        )\n",
    "        \n",
    "        # Build the neural network model\n",
    "        model = Sequential([\n",
    "            Dense(10, activation='relu', input_shape=(X_train.shape[1],)),  # Hidden layer with 10 nodes\n",
    "            Dense(1)  # Output layer with a single node (for regression task)\n",
    "        ])\n",
    "        \n",
    "        # Compile the model using Adam optimizer and mean squared error loss function\n",
    "        model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
    "        \n",
    "        # Train the model on the training data for 100 epochs\n",
    "        # Increasing the number of epochs allows the model to train longer, potentially improving accuracy\n",
    "        model.fit(X_train, y_train, epochs=100, verbose=0)\n",
    "        \n",
    "        # Evaluate the model on the test data\n",
    "        y_pred = model.predict(X_test, verbose=0)  # Predict on the test set\n",
    "        mse = mean_squared_error(y_test, y_pred)  # Calculate the Mean Squared Error\n",
    "        mse_list.append(mse)  # Append the MSE to the list\n",
    "    \n",
    "    # Return the mean and standard deviation of the MSEs\n",
    "    return np.mean(mse_list), np.std(mse_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f24a678-898e-4daf-a172-025cc901e097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to run the model with increased epochs and capture the results\n",
    "mean_c, std_c = increased_epochs_model(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cfe5e5d-1107-4368-85c3-3690a3b1f576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part C - Increased Epochs (100): Mean MSE = 170.7973410637508 Std MSE = 19.64843488987875\n"
     ]
    }
   ],
   "source": [
    "print(\"Part C - Increased Epochs (100): Mean MSE =\", mean_c, \"Std MSE =\", std_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626e1857-1d0f-4413-92b9-bdeb3e368bf2",
   "metadata": {},
   "source": [
    "#### How does the mean of the mean squared errors compare to that from Step B?\n",
    "\n",
    "The comparison between the Mean MSE values from Step B (Normalized Model) and Step C (Increased Epochs) shows the following:\n",
    "\n",
    "1. Mean MSE Comparison:\n",
    "    - Step B (Normalized Model): Mean MSE = 369.76\n",
    "    - Step C (Increased Epochs): Mean MSE = 170.80\n",
    "\n",
    "The mean MSE in Step C is significantly lower compared to Step B. This indicates that increasing the number of epochs (from 50 to 100) allowed the model to train for a longer period, enabling it to learn the data's underlying patterns more effectively, thereby improving prediction accuracy.\n",
    "\n",
    "2. Standard Deviation Comparison:\n",
    "    - Step B (Normalized Model): Std MSE = 93.85\n",
    "    - Step C (Increased Epochs): Std MSE = 19.65\n",
    "\n",
    "The standard deviation of MSE in Step C is also considerably lower than in Step B. This reduction in variability suggests that increasing the number of epochs not only improved accuracy but also made the model's performance more consistent across the 50 iterations.\n",
    "\n",
    "#### Conclusion:\n",
    "Increasing the number of epochs from 50 (Step B) to 100 (Step C) resulted in a substantial improvement in the model's accuracy (lower mean MSE) and stability (lower standard deviation of MSE). This shows that the model benefited from the additional training time to better capture the relationships within the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c52985-5b16-42da-9720-fb4303ea7b3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
