{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "205d6cce-1a8e-4678-a343-49c3a968b631",
   "metadata": {},
   "source": [
    "#### D. Increase the number of hidden layers (5 marks)\r\n",
    "\r\n",
    "Repeat part B but use a neural network with the following instead:\r\n",
    "\r\n",
    "- Three hidden layers, each of 10 nodes and ReLU activation function.\r\n",
    "\r\n",
    "How does the mean of the mean squared errors compare to that from Step B?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "495f44df-4bd4-49dc-a935-256007705078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np  # For numerical operations\n",
    "import pandas as pd  # For data manipulation\n",
    "\n",
    "from sklearn.metrics import mean_squared_error  # To calculate the Mean Squared Error\n",
    "from sklearn.model_selection import train_test_split  # To split the dataset into training and test sets\n",
    "from sklearn.preprocessing import StandardScaler  # To normalize the data\n",
    "from tensorflow.keras.layers import Dense  # To define the layers of the neural network\n",
    "from tensorflow.keras.models import Sequential  # To build the neural network\n",
    "from tensorflow.keras.optimizers import Adam  # Optimizer for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aba4c3d3-383f-4b2c-98df-c3038f9b2cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "url = \"concrete_data.csv\"  # Path to the dataset\n",
    "data = pd.read_csv(url)  # Load the dataset into a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa80ff98-e265-4165-bd9c-5d8c8ad94880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into predictors (X) and target variable (y)\n",
    "X = data.drop(\"Strength\", axis=1)  # Features/predictors (all columns except \"Strength\")\n",
    "y = data[\"Strength\"]  # Target variable (\"Strength\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25872414-26bb-4d38-ab8b-a5dc0d3d22a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part A: Build Baseline Model\n",
    "def baseline_model(X, y):\n",
    "    \"\"\"\n",
    "    Builds and evaluates a baseline regression model using Keras.\n",
    "    The model:\n",
    "    - Has one hidden layer with 10 nodes and ReLU activation.\n",
    "    - Uses the Adam optimizer and mean squared error loss function.\n",
    "    The process is repeated 50 times, and the mean and standard deviation of the MSEs are computed.\n",
    "\n",
    "    Parameters:\n",
    "    X: Features (predictors)\n",
    "    y: Target variable (concrete strength)\n",
    "\n",
    "    Returns:\n",
    "    Mean and standard deviation of the MSEs from 50 iterations.\n",
    "    \"\"\"\n",
    "    mse_list = []  # List to store MSEs from each iteration\n",
    "\n",
    "    # Repeat the process 50 times\n",
    "    for _ in range(50):\n",
    "        # Split the dataset into training (70%) and testing (30%) sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=np.random.randint(0, 100))\n",
    "        \n",
    "        # Build the neural network model\n",
    "        model = Sequential([\n",
    "            Dense(10, activation='relu', input_shape=(X_train.shape[1],)),  # Hidden layer with 10 nodes\n",
    "            Dense(1)  # Output layer with a single node (for regression)\n",
    "        ])\n",
    "        \n",
    "        # Compile the model using Adam optimizer and mean squared error loss\n",
    "        model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
    "        \n",
    "        # Train the model on the training data for 50 epochs\n",
    "        model.fit(X_train, y_train, epochs=50, verbose=0)\n",
    "        \n",
    "        # Evaluate the model on the test data\n",
    "        y_pred = model.predict(X_test, verbose=0)  # Predict on the test set\n",
    "        mse = mean_squared_error(y_test, y_pred)  # Calculate Mean Squared Error\n",
    "        mse_list.append(mse)  # Append the MSE to the list\n",
    "    \n",
    "    # Return the mean and standard deviation of the MSEs\n",
    "    return np.mean(mse_list), np.std(mse_list)\n",
    "\n",
    "# Part B: Normalize the data\n",
    "def normalized_model(X, y):\n",
    "    \"\"\"\n",
    "    Builds and evaluates a regression model using normalized data.\n",
    "    The normalization process:\n",
    "    - Subtracts the mean and divides by the standard deviation for each feature.\n",
    "    The rest of the process is the same as the baseline model, with 50 iterations.\n",
    "\n",
    "    Parameters:\n",
    "    X: Features (predictors)\n",
    "    y: Target variable (concrete strength)\n",
    "\n",
    "    Returns:\n",
    "    Mean and standard deviation of the MSEs from 50 iterations with normalized data.\n",
    "    \"\"\"\n",
    "    # Normalize the data using StandardScaler\n",
    "    scaler = StandardScaler()  # Initialize the scaler\n",
    "    X_normalized = scaler.fit_transform(X)  # Fit the scaler to X and transform it\n",
    "    \n",
    "    # Call the baseline model function with the normalized data\n",
    "    return baseline_model(X_normalized, y)\n",
    "\n",
    "# Part D: Increase the number of hidden layers\n",
    "def increased_layers_model(X, y):\n",
    "    \"\"\"\n",
    "    Builds and evaluates a regression model with three hidden layers.\n",
    "    The model:\n",
    "    - Has three hidden layers, each with 10 nodes and ReLU activation.\n",
    "    - Uses the Adam optimizer and mean squared error loss function.\n",
    "    The dataset is normalized before training, and the process is repeated 50 times.\n",
    "\n",
    "    Parameters:\n",
    "    X: Features (predictors)\n",
    "    y: Target variable (concrete strength)\n",
    "\n",
    "    Returns:\n",
    "    Mean and standard deviation of the MSEs from 50 iterations with increased hidden layers.\n",
    "    \"\"\"\n",
    "    mse_list = []  # List to store MSEs from each iteration\n",
    "\n",
    "    # Normalize the dataset to zero mean and unit variance\n",
    "    scaler = StandardScaler()  # Initialize the StandardScaler\n",
    "    X_normalized = scaler.fit_transform(X)  # Fit and transform X\n",
    "\n",
    "    # Repeat the process 50 times for robust evaluation\n",
    "    for _ in range(50):\n",
    "        # Split the normalized dataset into training and testing sets (70% train, 30% test)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_normalized, y, test_size=0.3, random_state=np.random.randint(0, 100)\n",
    "        )\n",
    "        \n",
    "        # Build the neural network model with three hidden layers\n",
    "        model = Sequential([\n",
    "            Dense(10, activation='relu', input_shape=(X_train.shape[1],)),  # First hidden layer with 10 nodes\n",
    "            Dense(10, activation='relu'),  # Second hidden layer with 10 nodes\n",
    "            Dense(10, activation='relu'),  # Third hidden layer with 10 nodes\n",
    "            Dense(1)  # Output layer with a single node (for regression task)\n",
    "        ])\n",
    "        \n",
    "        # Compile the model using Adam optimizer and mean squared error loss function\n",
    "        model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
    "        \n",
    "        # Train the model on the training data for 50 epochs\n",
    "        # Adding more layers increases the model's complexity, which can improve its ability to learn complex patterns\n",
    "        model.fit(X_train, y_train, epochs=50, verbose=0)\n",
    "        \n",
    "        # Evaluate the model on the test data\n",
    "        y_pred = model.predict(X_test, verbose=0)  # Predict on the test set\n",
    "        mse = mean_squared_error(y_test, y_pred)  # Calculate the Mean Squared Error\n",
    "        mse_list.append(mse)  # Append the MSE to the list\n",
    "    \n",
    "    # Return the mean and standard deviation of the MSEs\n",
    "    return np.mean(mse_list), np.std(mse_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "411c3f57-42e0-4621-a4b1-93e84b2e084d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to run the model with increased hidden layers and capture the results\n",
    "mean_d, std_d = increased_layers_model(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f173d36-e5de-4084-93d2-17bbcfe6d33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part D - Increased Hidden Layers: Mean MSE = 129.39389660207763 Std MSE = 13.736224909535\n"
     ]
    }
   ],
   "source": [
    "print(\"Part D - Increased Hidden Layers: Mean MSE =\", mean_d, \"Std MSE =\", std_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d06dc7-ba82-4766-a9ea-277b230978c7",
   "metadata": {},
   "source": [
    "### **Comparison of Results:**\n",
    "\n",
    "| **Model**                | **Mean MSE** | **Std MSE** | **Observations**                                                                                                                                       |\n",
    "|---------------------------|--------------|-------------|---------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **Part A - Baseline**     | 283.68       | 236.16      | Baseline performance with unnormalized data. Shows moderate accuracy but high variability, indicating inconsistent performance across iterations.       |\n",
    "| **Part B - Normalized**   | 369.76       | 93.85       | Normalization increases variability in predictions (higher Mean MSE), but significantly reduces Std MSE, showing more consistent model behavior.         |\n",
    "| **Part C - Increased Epochs** | 170.80       | 19.65       | Training for 100 epochs improves accuracy (lower Mean MSE) and reduces variability significantly, suggesting better learning of the data patterns.       |\n",
    "| **Part D - Increased Hidden Layers** | 129.39       | 13.74       | Adding more hidden layers further improves accuracy (lowest Mean MSE) and stabilizes performance (lowest Std MSE), indicating better model complexity.   |\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusions:**\n",
    "\n",
    "1. **Baseline Model (Part A):**\n",
    "   - The baseline model achieves a **Mean MSE of 283.68**, but its high **Std MSE of 236.16** reflects inconsistent performance.\n",
    "   - This suggests the need for further optimization (e.g., normalization, training adjustments, or architecture changes).\n",
    "\n",
    "2. **Normalized Model (Part B):**\n",
    "   - Normalizing the data leads to a **higher Mean MSE (369.76)** but a much **lower Std MSE (93.85)**.\n",
    "   - The trade-off indicates that normalization improves model consistency but may require other adjustments (like epochs or layers) to achieve better accuracy.\n",
    "\n",
    "3. **Increased Epochs (Part C):**\n",
    "   - Training for 100 epochs significantly reduces the **Mean MSE to 170.80** and the **Std MSE to 19.65**.\n",
    "   - This shows that extending the training allows the model to learn better patterns, resulting in both improved accuracy and stability.\n",
    "\n",
    "4. **Increased Hidden Layers (Part D):**\n",
    "   - Adding three hidden layers achieves the **best performance** with the **lowest Mean MSE (129.39)** and **lowest Std MSE (13.74)**.\n",
    "   - This indicates that increasing model depth enables the network to better capture complex relationships in the data, improving both accuracy and consistency.\n",
    "\n",
    "---\n",
    "\n",
    "### **Overall Summary:**\n",
    "\n",
    "- **Accuracy (Mean MSE):**  \n",
    "  The performance improves progressively from Part A to Part D, with the lowest Mean MSE observed in Part D. This shows that enhancing the model through additional training epochs and hidden layers improves its predictive capability.\n",
    "\n",
    "- **Stability (Std MSE):**  \n",
    "  The standard deviation of MSE decreases across the parts, with the lowest Std MSE in Part D. This suggests that increasing training time and network depth stabilizes model predictions.\n",
    "\n",
    "- **Best Model:**  \n",
    "  **Part D (Increased Hidden Layers)** outperforms all others in both accuracy and consistency, making it the most effective configuration for this task.\n",
    "\n",
    "---\n",
    "\n",
    "### **Recommendations:**\n",
    "\n",
    "- Use **normalized data**, extend training epochs, and design a deeper model (with multiple hidden layers) for optimal performance.\n",
    "- Future experimentation could explore further hyperparameter tuning (e.g., number of layers, nodes, learning rate) or regularization techniques to refine the model further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5690c066-a615-4fa5-a4fa-f06f80ae4e83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
